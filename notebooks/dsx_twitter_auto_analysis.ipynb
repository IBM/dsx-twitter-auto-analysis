{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Sentiment Analysis\n",
    "\n",
    "Twitter tweets are an attractive data source to analyze\n",
    "as Twitter users tweet about many different topics in which\n",
    "they not only impart knowledge to others but also express\n",
    "their feelings and opinions. Analyzing this data can result\n",
    "in valuable insights and can be useful to detect trends and\n",
    "drive business decisions. Notebooks are a powerful platform\n",
    "for data scientists to analyze Twitter data.\n",
    "\n",
    "## Auto Industry Tweets\n",
    "\n",
    "This notebook analyzes Twitter data to glean insights about\n",
    "the automotive industry. As the automotive industry is one of\n",
    "the largest industries in the world and still very much a growth\n",
    "industry, analyzing tweets about cars can assist manufacturers\n",
    "to pay closer attention to market dynamics and position their\n",
    "companies to take advantage of demographic changes and shifts\n",
    "in consumer expectations.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "The notebook is structured into different sections. In the first\n",
    "sections, you will perform a general analysis on the data set then\n",
    "you will go deeper in the analysis to gain meaningful insights\n",
    "about manufacturers.\n",
    "\n",
    "1. Determine the countries with the highest number of tweets\n",
    "   (based on the user profile information).\n",
    "\n",
    "2. Analyze tweet sentiments\n",
    "\n",
    "3. Draw insights from tweets about major car manufacturers worldwide\n",
    "   by combining Twitter timeline analysis with sentiment, gender\n",
    "   distribution and location distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "#\n",
    "# The hidden_cell magic allows us to avoid exposing credentials in shared\n",
    "# output, but you still should be careful how you save, share or allow access\n",
    "# to the notebook.\n",
    "\n",
    "# SET-UP: \n",
    "#\n",
    "#  1) Set credential_map to your inserted credentials dict (see below).\n",
    "#     It might be named credentials_1 or something else.\n",
    "#  2) Edit the DASHDB_PREFIX to match the prefix you used when loading data.\n",
    "\n",
    "# Use the \"Find and Add Data\" upper right 10-over-01 icon and the\n",
    "# \"Connections\" tab and click \"Insert to code\" to add dashDB data connection\n",
    "# credentials here. Then use the credential dict (the name may be different)\n",
    "# to set your credentials in the next cell. If you didn't setup a datasource\n",
    "# in the project. You can edit in the individual strings instead.\n",
    "\n",
    "# Insert connection to dashDB here:\n",
    "credential_map = None\n",
    "\n",
    "if not credential_map:\n",
    "    raise Exception(\"dashDB credentials need to be added.\")\n",
    "\n",
    "DASHDB_PREFIX = \"CARS2015\"\n",
    "\n",
    "DASHDB_USERNAME = credential_map['username']\n",
    "DASHDB_PASSWORD = credential_map['password']\n",
    "DASHDB_SCHEMA = credential_map['username'].upper()\n",
    "DASHDB_JDBC_URL = credential_map['ssljdbcurl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture captured_io\n",
    "\n",
    "# Captured_io magic is used here so that the output of the pip install\n",
    "# does not have to show in the final report. To check the output, run\n",
    "# the next cell with DEBUG=False. This install may require restarting\n",
    "# the kernel and if you restart the kernel, then you should start\n",
    "# at the top of the notebook again.\n",
    "\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can use this cell to see the captured IO from the above cell.\n",
    "# After seeing that the captured_io from above is OK, then you can set\n",
    "# DEBUG = False (and re-run this cell) to make the final report look cleaner.\n",
    "# DEBUG will also be used below to hide some output that you may want while\n",
    "# getting started, but may not want in the final exported results.\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    captured_io()  # noqa F821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and initialize modules\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from operator import add\n",
    "import re\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from IPython.core.display import Javascript\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sqlContext = SQLContext(sc)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining global variables and helper functions\n",
    "#\n",
    "# car_makers_list: An array of car manufacturers. Each element in the array is\n",
    "#                  a list with all spelling variants.\n",
    "# car_makers_name_list: List of the preferred uppercase names to use for car\n",
    "#                       makers.\n",
    "# num_car_makers: The number of car makers expected.\n",
    "\n",
    "car_makers_list = [['bmw'],\n",
    "                   ['daimler', 'mercedes'],\n",
    "                   ['gm', 'general motors'],\n",
    "                   ['tesla'],\n",
    "                   ['toyota'],\n",
    "                   ['vw', 'volkswagen']]\n",
    "num_car_makers = len(car_makers_list)\n",
    "\n",
    "car_makers_name_list = []\n",
    "for car_maker in car_makers_list:\n",
    "    car_makers_name_list.append(car_maker[0].upper())\n",
    "\n",
    "# Plotting variables\n",
    "ind = np.arange(num_car_makers)  # index list for plotting\n",
    "width = 0.8  # the width of the bars in the bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# The helper function called GeoChart is used to plot the world map in a DOM\n",
    "# element (an iframe).\n",
    "def GeoChart(data_string, element):\n",
    "    return Javascript(\"\"\"\n",
    "        //container.show();\n",
    "        function draw() {{\n",
    "          var chart = new google.visualization.GeoChart(document.getElementById(\"\"\" + element + \"\"\"));\n",
    "          chart.draw(google.visualization.arrayToDataTable(\"\"\" + data_string + \"\"\"));\n",
    "        }}\n",
    "        google.load('visualization', '1.0',\n",
    "                    {'callback': draw, 'packages':['geochart']});\n",
    "        \"\"\", lib=\"https://www.google.com/jsapi\")\n",
    "\n",
    "\n",
    "# The helper function addMissingDates checks for any missing dates in\n",
    "# DataFrames with time series data.\n",
    "#   * baseDataframe: This DataFrame contains all dates. It must have the\n",
    "#                    column names [POSTING_TIME, NUM_TWEETS]\n",
    "#   * checkedDataframe: This DataFrame contains the dates that need to be\n",
    "#                       checked. It must have the column names\n",
    "#                       [POSTING_TIME, NUM_TWEETS]\n",
    "def addMissingDates(baseDates, checkedDates):\n",
    "    temp = checkedDates.copy()\n",
    "    checkedDatesValues = checkedDates['POSTING_TIME']\n",
    "    for index, row in baseDates.iterrows():\n",
    "        if not row['POSTING_TIME'] in checkedDatesValues.tolist():\n",
    "            row['NUM_TWEETS'] = 0\n",
    "            temp = temp.append(row)\n",
    "    return temp.sort_values('POSTING_TIME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading data into Spark DataFrames\n",
    "#\n",
    "# To access the data in dashDB, you must provide the dashDB service\n",
    "# credentials. These were set in the first input cell to make them obvious\n",
    "# and easy to find. The data is retrieved from the database by using the\n",
    "# Spark JDBC connector and is loaded into a Spark DataFrame in the notebook\n",
    "# called df_CARS_TWEETS using sqlContext.read.jdbc. The dataframe has the same\n",
    "# column names as the tweets table in dashDB.\n",
    "\n",
    "props = {}\n",
    "props['user'] = DASHDB_USERNAME\n",
    "props['password'] = DASHDB_PASSWORD\n",
    "jdbcurl = DASHDB_JDBC_URL\n",
    "\n",
    "# Get the data frame\n",
    "df_TWEETS = sqlContext.read.jdbc(jdbcurl, DASHDB_SCHEMA + '.' +\n",
    "                                 DASHDB_PREFIX + '_' + 'TWEETS',\n",
    "                                 properties=props)\n",
    "df_SENTIMENTS = sqlContext.read.jdbc(jdbcurl, DASHDB_SCHEMA + '.' +\n",
    "                                     DASHDB_PREFIX + '_' + 'SENTIMENTS',\n",
    "                                     properties=props)\n",
    "\n",
    "if DEBUG:\n",
    "    df_TWEETS.printSchema()\n",
    "    df_SENTIMENTS.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType, StructField, StructType, TimestampType  # noqa\n",
    "udf = UserDefinedFunction(\n",
    "    lambda x: 0 if x == 'AMBIVALENT' else 1 if x == 'POSITIVE' else -1,\n",
    "    IntegerType())\n",
    "udf2 = UserDefinedFunction(\n",
    "    lambda x: 'POSITIVE' if x > 0 else 'NEGATIVE' if x < 0 else 'AMBIVALENT',\n",
    "    StringType())\n",
    "\n",
    "df = df_TWEETS.join(df_SENTIMENTS, \"MESSAGE_ID\")\n",
    "df = df.withColumn('SENTIMENT_POLARITY', udf(df.SENTIMENT_POLARITY)).groupBy(\n",
    "    'MESSAGE_ID').agg(F.mean('SENTIMENT_POLARITY').alias(\"SENTIMENT_POLARITY\"))\n",
    "df = df.withColumn('SENTIMENT', udf2(df.SENTIMENT_POLARITY))\n",
    "df_JOIN_TWEETS = df_TWEETS.join(df, \"MESSAGE_ID\")\n",
    "\n",
    "if DEBUG:\n",
    "    df_JOIN_TWEETS.printSchema()\n",
    "    \n",
    "# The following code cell counts the number of rows which were loaded into the\n",
    "# DataFrame which is equivalent to the number of tweets available for further\n",
    "# processing.\n",
    "print(\"Number of tweets: \" + str(df_TWEETS.count()))\n",
    "print(\"Number of sentiment records: \" + str(df_SENTIMENTS.count()))    \n",
    "print(\n",
    "    \"Aggregated tweets with sentiment scores: \" + str(df_JOIN_TWEETS.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforming the data\n",
    "#\n",
    "# You can't analyze the data that you have just loaded into the data frames\n",
    "# the way it is. You must first mung the data. The output of the data\n",
    "# transformation process is a new Spark DataFrame which has the target\n",
    "# structure on which to base the data analysis. This Spark DataFrame called\n",
    "# df_cleaned_tweets functions as the main data source for all further\n",
    "# processing.\n",
    "#\n",
    "# Run the next cell to carry out the following transformations on the data:\n",
    "# \n",
    "# 1) Remove the time from the timestamp values as only the date information is\n",
    "#    relevant.\n",
    "#\n",
    "# 2) Change the values in the string columns like user country, state and city\n",
    "#    to upper case.\n",
    "#\n",
    "# 3) Change the tweet posting location information from a string\n",
    "#    ('pos (42.000 42.000)') to a numeric value represented by the longitude\n",
    "#     and latitude coordinates. \n",
    "#\n",
    "# You will use the resulting dataframe (df_cleaned_tweets) as the base data\n",
    "# source for all further step. The sample uses Spark to do all heavy\n",
    "# computation. When it is time to plot or collect the results, the returned\n",
    "# data is copied into the kernel memory, in other words moved from a Spark\n",
    "# DataFrame to a pandas DataFrame.\n",
    "\n",
    "\n",
    "def getLongitudeLatitude(position):\n",
    "    parts = str(position).split('(')[1].split(')')[0].split(' ')\n",
    "    return parts\n",
    "\n",
    "\n",
    "def getLongitude(row):\n",
    "    if row.MESSAGE_LOCATION is None:\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            parts = getLongitudeLatitude(row.MESSAGE_LOCATION)\n",
    "            lon = float(parts[0])\n",
    "            return lon\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "def getLatitude(row):\n",
    "    if row.MESSAGE_LOCATION is None:\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            parts = getLongitudeLatitude(row.MESSAGE_LOCATION)\n",
    "            lon = float(parts[1])\n",
    "            return lon\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def getDateIgnoreTime(row):\n",
    "    posting_time = parser.parse(str(row.MESSAGE_POSTED_TIME))\n",
    "    posting_time = posting_time.replace(\n",
    "        hour=0, minute=0, second=0, microsecond=0)\n",
    "    return posting_time\n",
    "\n",
    "\n",
    "schema = StructType([StructField('MESSAGE_ID', StringType()),\n",
    "                     StructField('POSTING_TIME', TimestampType()),\n",
    "                     StructField('MESSAGE_BODY', StringType()),\n",
    "                     StructField('USER_GENDER', StringType()),\n",
    "                     StructField('USER_STATE', StringType()),\n",
    "                     StructField('USER_COUNTRY', StringType()),\n",
    "                     StructField('USER_CITY', StringType()),\n",
    "                     StructField('MESSAGE_LANGUAGE', StringType()),\n",
    "                     StructField('MESSAGE_LOCATION_LONGITUDE', FloatType()),\n",
    "                     StructField('MESSAGE_LOCATION_LATITUDE', FloatType()),\n",
    "                     StructField('SENTIMENT', StringType()),\n",
    "                     StructField('USER_FOLLOWERS_COUNT', IntegerType()),\n",
    "                     StructField('USER_FRIENDS_COUNT', IntegerType())])\n",
    "\n",
    "df_cleaned_tweets = sqlContext.createDataFrame(df_JOIN_TWEETS.map(lambda row: [\n",
    "    row.MESSAGE_ID,\n",
    "    getDateIgnoreTime(row),\n",
    "    row.MESSAGE_BODY,\n",
    "    row.USER_GENDER,\n",
    "    unicode(row.USER_STATE).upper(),\n",
    "    unicode(row.USER_COUNTRY).upper(),\n",
    "    unicode(row.USER_CITY).upper(),\n",
    "    row.MESSAGE_LANGUAGE,\n",
    "    getLongitude(row),\n",
    "    getLatitude(row),\n",
    "    row.SENTIMENT,\n",
    "    row.USER_FOLLOWERS_COUNT,\n",
    "    row.USER_FRIENDS_COUNT\n",
    "]), schema)\n",
    "\n",
    "df_cleaned_tweets.registerTempTable(DASHDB_PREFIX + '_TWEETS_CLEANED')\n",
    "if DEBUG:\n",
    "    df_cleaned_tweets.printSchema()\n",
    "df_cleaned_tweets.cache();  # End in semi-colon to suppress output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of tweets by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This section shows you how to extract the countries, which have the highest\n",
    "# number of tweets. To do that the data is grouped according to the\n",
    "# USER_COUNTRY column and the rows in each group are counted. Then the\n",
    "# groups are sorted in descending order.\n",
    "\n",
    "# Group by country\n",
    "df_cleaned_tweets_countries = df_cleaned_tweets.groupBy(\n",
    "    'USER_COUNTRY').agg(\n",
    "    F.count('MESSAGE_BODY').alias('NUM_TWEETS')).orderBy(\n",
    "    'NUM_TWEETS', ascending=False)\n",
    "\n",
    "df_cleaned_tweets_countries.cache();  # End in semi-colon to suppress output\n",
    "\n",
    "# Show tweet count for 5 top countries\n",
    "if DEBUG:\n",
    "    df_cleaned_tweets_countries.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_df_cleaned_tweets_countries = df_cleaned_tweets_countries.toPandas()\n",
    "p_df_cleaned_tweets_countries.ix[\n",
    "    p_df_cleaned_tweets_countries['USER_COUNTRY'] == 'NONE', 'USER_COUNTRY'\n",
    "] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The next cells show you two ways of plotting the countries with the largest\n",
    "# number of tweets. As this data needs no further munging, the data is copied\n",
    "# into a pandas DataFrame which is used for plotting the results.\n",
    "\n",
    "num_plotted_countries = 10\n",
    "\n",
    "countries = p_df_cleaned_tweets_countries[\n",
    "    'USER_COUNTRY'][:num_plotted_countries]\n",
    "num_tweets = p_df_cleaned_tweets_countries[\n",
    "    'NUM_TWEETS'][:num_plotted_countries]\n",
    "y_pos = np.arange(len(countries))\n",
    "colors = np.repeat('b', num_plotted_countries - 1).tolist()\n",
    "colors = ['gray'] + colors\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(y_pos, num_tweets, align='center', color=colors)\n",
    "plt.yticks(y_pos, countries)\n",
    "plt.xlabel('Number of Tweets')\n",
    "plt.title('Tweets Country Distribution based on the User Profile')\n",
    "plt.ylim(-1, len(y_pos))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%html\n",
    "<div id=\"plot_div\" style=\"width: 900px; height: 500px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = p_df_cleaned_tweets_countries['USER_COUNTRY']\n",
    "num_tweets = p_df_cleaned_tweets_countries['NUM_TWEETS']\n",
    "\n",
    "data = \"[['Country', 'Num Tweets']\"\n",
    "index = 0\n",
    "for country in countries:\n",
    "    country = country.replace(\"'\", \"\")\n",
    "    data = data + \", ['\" + country + \"', \" + str(num_tweets[index]) + \"]\"\n",
    "    index += 1\n",
    "data += \"]\"\n",
    "\n",
    "GeoChart(data, \"'plot_div'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As you won't need the data frames related to countries any longer run the\n",
    "# following cell to clear the memory of those variables (both Spark and\n",
    "# pandas).\n",
    "\n",
    "df_cleaned_tweets_countries.unpersist()\n",
    "df_cleaned_tweets_countries = None\n",
    "p_df_cleaned_tweets_countries = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing tweet sentiments\n",
    "\n",
    "Sentiment analysis is one of the most valuable sources of information\n",
    "that the IBM twitter API provides. By giving each tweet a sentiment\n",
    "value, you can determine whether the content of a tweet is positive,\n",
    "negative, ambivalent, neutral, or NULL, if no value is provided by\n",
    "the API. Unfortunately, a sentiment value is provided for English,\n",
    "German, French, and Spanish tweets only. As the data set also has tweets\n",
    "in other languages, only a subset of the tweets in the data set have\n",
    "a sentiment value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get number of tweets with P N U sentiment by grouping the sentiment value.\n",
    "tweets_grouped_by_sentiment = df_cleaned_tweets.groupBy(\n",
    "    'SENTIMENT').agg(F.count('MESSAGE_ID').alias('NUM_TWEETS'))\n",
    "\n",
    "tweets_grouped_by_sentiment.cache()\n",
    "\n",
    "# Show the tweet counts by sentiment (text output)\n",
    "if DEBUG:\n",
    "    tweets_grouped_by_sentiment.show(5)\n",
    "\n",
    "# Move the results to pandas\n",
    "p_tweets_grouped_by_sentiment = tweets_grouped_by_sentiment.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell plots the sentiment values of all tweets in the data set.\n",
    "\n",
    "# Data plot 1\n",
    "plot1_labels = p_tweets_grouped_by_sentiment['SENTIMENT']\n",
    "plot1_values = p_tweets_grouped_by_sentiment['NUM_TWEETS']\n",
    "plot1_colors = ['blue', 'red', 'gray', 'yellow', 'green']\n",
    "\n",
    "# Data plot 2\n",
    "cond1 = (p_tweets_grouped_by_sentiment['SENTIMENT'] == 'POSITIVE')\n",
    "cond2 = (p_tweets_grouped_by_sentiment['SENTIMENT'] == 'NEGATIVE')\n",
    "\n",
    "pMessage_sentiment_statistics_defined = p_tweets_grouped_by_sentiment[\n",
    "    cond1 | cond2]\n",
    "plot2_labels = pMessage_sentiment_statistics_defined['SENTIMENT']\n",
    "plot2_values = pMessage_sentiment_statistics_defined['NUM_TWEETS']\n",
    "plot2_colors = ['blue', 'red']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(23, 10))\n",
    "\n",
    "axes[0].pie(\n",
    "    plot1_values, labels=plot1_labels, colors=plot1_colors, autopct='%1.1f%%')\n",
    "axes[0].set_title('Percentage of Sentiment Values in all Tweets')\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].legend(loc=\"upper right\", labels=plot1_labels)\n",
    "\n",
    "# Plot\n",
    "axes[1].pie(\n",
    "    plot2_values, labels=plot2_labels, colors=plot2_colors, autopct='%1.1f%%')\n",
    "axes[1].set_title(\n",
    "    'Percentage of Positive and Negative Sentiment Values in all Tweets')\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].legend(loc=\"upper right\", labels=plot2_labels)\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After you have plotted the results, run this cell to release the memory of\n",
    "# the variables you used.\n",
    "\n",
    "tweets_grouped_by_sentiment.unpersist()\n",
    "tweets_grouped_by_sentiment = None\n",
    "p_tweets_grouped_by_sentiment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing tweet timelines\n",
    "\n",
    "To learn more about which car manufacturing events occurred in 2015, you can\n",
    "plot data over time. The following section groups all tweets created in 2015\n",
    "by their posting date (and sentiment value) and counts the number of tweets\n",
    "per date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group by year-month-day and the sentiment\n",
    "df_tweets_and_sentiment_over_time = df_cleaned_tweets.groupBy(\n",
    "    'POSTING_TIME', 'SENTIMENT').agg(\n",
    "    F.count('MESSAGE_BODY').alias('NUM_TWEETS')).orderBy(\n",
    "    'POSTING_TIME', ascending=True)\n",
    "\n",
    "# Group by year-month-day\n",
    "df_tweets_over_time = df_tweets_and_sentiment_over_time.groupBy(\n",
    "    'POSTING_TIME').agg(\n",
    "    F.sum('NUM_TWEETS').alias('NUM_TWEETS')).orderBy(\n",
    "    'POSTING_TIME', ascending=True)\n",
    "\n",
    "# Move to Pandas\n",
    "p_df_tweets_and_sentiment_over_time = (\n",
    "    df_tweets_and_sentiment_over_time.toPandas())\n",
    "p_df_tweets_over_time = df_tweets_over_time.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the next cell to prepare the data for plotting by moving data with\n",
    "# different sentiment values into different data frames.\n",
    "\n",
    "p = p_df_tweets_and_sentiment_over_time\n",
    "\n",
    "positive_mask = p['SENTIMENT'] == 'POSITIVE'\n",
    "negative_mask = p['SENTIMENT'] == 'NEGATIVE'\n",
    "neutral_mask = p['SENTIMENT'] == 'NEUTRAL'\n",
    "ambivalent_mask = p['SENTIMENT'] == 'AMBIVALENT'\n",
    "null_mask = p['SENTIMENT'].isnull()\n",
    "\n",
    "p_df_tweets_and_sentiment_over_time_positive = p[positive_mask]\n",
    "p_df_tweets_and_sentiment_over_time_negative = p[negative_mask]\n",
    "p_df_tweets_and_sentiment_over_time_neutral = p[neutral_mask]\n",
    "p_df_tweets_and_sentiment_over_time_ambivalent = p[ambivalent_mask]\n",
    "p_df_tweets_and_sentiment_over_time_null = p[null_mask]\n",
    "\n",
    "p_df_num_tweets_and_sentiment_over_time_positive = addMissingDates(\n",
    "    p_df_tweets_over_time, p_df_tweets_and_sentiment_over_time_positive)\n",
    "p_df_num_tweets_and_sentiment_over_time_negative = addMissingDates(\n",
    "    p_df_tweets_over_time, p_df_tweets_and_sentiment_over_time_negative)\n",
    "p_df_num_tweets_and_sentiment_over_time_neutral = addMissingDates(\n",
    "    p_df_tweets_over_time, p_df_tweets_and_sentiment_over_time_neutral)\n",
    "p_df_num_tweets_and_sentiment_over_time_ambivalent = addMissingDates(\n",
    "    p_df_tweets_over_time, p_df_tweets_and_sentiment_over_time_ambivalent)\n",
    "p_df_num_tweets_and_sentiment_over_time_null = addMissingDates(\n",
    "    p_df_tweets_over_time, p_df_tweets_and_sentiment_over_time_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the next cell to plot your results after data munging. First plot the\n",
    "# number of tweets about Volkswagen, Toyota, BMW, Daimler, and General\n",
    "# Motors spread across 2015, then plot their positive and negative sentiment\n",
    "# values, and lastly plot only the sentiment value applied to the number of\n",
    "# tweets.\n",
    "\n",
    "# Take the beginning of each month\n",
    "mask_1day = p_df_tweets_over_time['POSTING_TIME'].map(lambda x: x.day) == 1\n",
    "x = p_df_tweets_over_time[mask_1day]['POSTING_TIME']\n",
    "y = p_df_tweets_over_time['NUM_TWEETS']\n",
    "\n",
    "# Positive preparation\n",
    "py = p_df_num_tweets_and_sentiment_over_time_positive['NUM_TWEETS']\n",
    "# Negative preparation\n",
    "ny = p_df_num_tweets_and_sentiment_over_time_negative['NUM_TWEETS']\n",
    "# Undefined preparation\n",
    "ney = p_df_num_tweets_and_sentiment_over_time_neutral['NUM_TWEETS']\n",
    "# Ambivalent preparation\n",
    "ay = p_df_num_tweets_and_sentiment_over_time_ambivalent['NUM_TWEETS']\n",
    "# Null preparation - undefined\n",
    "nully = p_df_num_tweets_and_sentiment_over_time_null['NUM_TWEETS']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
    "\n",
    "# Plot1\n",
    "axes[0].plot(range(len(y)), y, linewidth=2)\n",
    "axes[0].set_xticks(x.index.tolist())\n",
    "axes[0].set_xticklabels([day.strftime(\"%Y-%m-%d\") for day in x])\n",
    "axes[0].margins = 0\n",
    "axes[0].set_xlabel('Date/Time')\n",
    "axes[0].set_ylabel('Num of Tweets')\n",
    "axes[0].set_title('Number of Tweets Over Time - ALL TWEETS')\n",
    "axes[0].set_xlim(0, len(y))\n",
    "axes[0].legend(loc=\"upper right\", labels=['All Tweets'])\n",
    "\n",
    "axes[1].plot(range(len(y)), y, linewidth=2, color='blue')\n",
    "axes[1].plot(range(len(py)), py, linewidth=2, color='green')\n",
    "axes[1].plot(range(len(ny)), ny, linewidth=2, color='red')\n",
    "\n",
    "axes[1].set_xticks(x.index.tolist())\n",
    "axes[1].set_xticklabels([day.strftime(\"%Y-%m-%d\") for day in x])\n",
    "axes[1].margins = 0\n",
    "axes[1].set_xlabel('Date/Time')\n",
    "axes[1].set_ylabel('Num of Tweets')\n",
    "axes[1].set_title('Number of Tweets Over Time - All, Positive and Negative')\n",
    "axes[1].set_xlim(0, len(y))\n",
    "axes[1].legend(loc=\"upper right\",\n",
    "               labels=[\n",
    "                   'All Tweets', 'Positive', 'Negative', 'Undefined Sentiment'\n",
    "               ])\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After you've completed plotting the results, clear the memory of the\n",
    "# variables you used.\n",
    "\n",
    "df_tweets_and_sentiment_over_time.unpersist()\n",
    "df_tweets_over_time.unpersist()\n",
    "\n",
    "df_tweets_and_sentiment_over_time = None\n",
    "df_tweets_over_time = None\n",
    "\n",
    "p_df_tweets_and_sentiment_over_time = None\n",
    "# We will need the variable p_df_tweets_over_time\n",
    "\n",
    "p_df_tweets_and_sentiment_over_time_positive = None\n",
    "p_df_tweets_and_sentiment_over_time_negative = None\n",
    "p_df_tweets_and_sentiment_over_time_neutral = None\n",
    "p_df_tweets_and_sentiment_over_time_ambivalent = None\n",
    "p_df_tweets_and_sentiment_over_time_null = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights from tweets about car manufacturers\n",
    "\n",
    "This section combines different types of analyses to dig deeper\n",
    "into the list of car manufacturers (Volkswagen, Toyota, BMW,\n",
    "Daimler, and  General Motors). The purpose of the analyses is to\n",
    "obtain car manufacturer-based insights from tweets that could be\n",
    "interesting and useful to detect potential car buyers. The first\n",
    "step is to detect the tweets that mention certain car manufacturers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to munge the data into a new data frame that has additional\n",
    "# columns for each car manufacturer. The value in this column in each row\n",
    "# indicates whether the car company was mentioned in a tweet or not. \n",
    "#\n",
    "# This new data frame is the new data source for subsequent computations.\n",
    "\n",
    "\n",
    "def hasWord(message, word):\n",
    "    return word in message\n",
    "\n",
    "\n",
    "def checkCarMaker(message):\n",
    "    tmp = []\n",
    "    for car_maker_list_var in car_makers_list:\n",
    "        contain = False\n",
    "        for car_maker in car_maker_list_var:\n",
    "            if hasWord(message, car_maker):\n",
    "                contain = True\n",
    "        tmp.extend([contain])\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def checkCarFeatures(message, feature_list):\n",
    "    tmp = []    \n",
    "    contain = False\n",
    "    for term in feature_list:\n",
    "        if hasWord(message, term.decode('utf8')):\n",
    "            contain = True\n",
    "    tmp.extend([contain])\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def getInfluence(tweet):\n",
    "    return (tweet.USER_FOLLOWERS_COUNT + tweet.USER_FRIENDS_COUNT) / 2\n",
    "\n",
    "\n",
    "def getAllAttributes(tweet):\n",
    "    message = unicode(tweet.MESSAGE_BODY).lower()\n",
    "    \n",
    "    # Message id and line\n",
    "    tmp = [tweet.MESSAGE_ID, tweet.MESSAGE_BODY, tweet.SENTIMENT,\n",
    "           tweet.USER_GENDER, unicode(tweet.USER_COUNTRY).upper(),\n",
    "           tweet.POSTING_TIME, getInfluence(tweet)]\n",
    "\n",
    "    # Competitors in line\n",
    "    tmp.extend(checkCarMaker(message)) \n",
    "    return tmp\n",
    "\n",
    "\n",
    "columns_names = ['MESSAGE_ID', 'MESSAGE_BODY', 'SENTIMENT', 'USER_GENDER',\n",
    "                 'USER_COUNTRY', 'POSTING_TIME', 'INFLUENCE']\n",
    "for carMakerName in car_makers_name_list:\n",
    "        columns_names.append(carMakerName)\n",
    "\n",
    "df_tweets_car_maker = sqlContext.createDataFrame(df_cleaned_tweets.map(\n",
    "    lambda x: getAllAttributes(x)), columns_names)\n",
    "\n",
    "df_tweets_car_maker.cache();\n",
    "\n",
    "if DEBUG:\n",
    "    df_tweets_car_maker.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analyzing tweet timelines by car maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You will plot the data over time according to each car manufacturer. This\n",
    "# can be done by using the Spark data frame that you just constructed. You\n",
    "# will filter the data by each car maker and then regroup the resulting\n",
    "# data frame according to the posting time of the tweets.\n",
    "\n",
    "if DEBUG:\n",
    "    print('Calculating the number of tweets over 2015 for each car maker:')\n",
    "\n",
    "car_maker_results_list = []\n",
    "for car_maker in car_makers_name_list:\n",
    "    # Get car maker dataframe\n",
    "    df_car_maker = df_tweets_car_maker.filter(\n",
    "        df_tweets_car_maker[car_maker] == True)\n",
    "    overall_car_maker_time_data = df_car_maker.groupBy(\n",
    "        'POSTING_TIME').agg(\n",
    "        F.count('MESSAGE_ID').alias('COUNT')).orderBy(\n",
    "        'POSTING_TIME', ascending=True)\n",
    "\n",
    "    p_overall_car_maker_time_data = overall_car_maker_time_data.toPandas()\n",
    "    car_maker_results_list.append(p_overall_car_maker_time_data)\n",
    "    \n",
    "    overall_car_maker_time_data.unpersist()\n",
    "    \n",
    "    # Print to show progress while cell runs\n",
    "    if DEBUG:\n",
    "        print('Done for ' + car_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask_1day = p_df_tweets_over_time['POSTING_TIME'].map(lambda x: x.day) == 1\n",
    "x = p_df_tweets_over_time[mask_1day]['POSTING_TIME']\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\n",
    "colors = ['blue', 'red', 'green', 'yellow', 'pink', 'black']\n",
    "\n",
    "for i in range(0, len(car_makers_name_list)):\n",
    "    y = car_maker_results_list[i]['COUNT']\n",
    "    axes.plot(range(len(y)), y, linewidth=2, color=colors[i])\n",
    "\n",
    "axes.set_xticks(x.index.tolist())\n",
    "axes.set_xticklabels([day.strftime(\"%Y-%m-%d\") for day in x])\n",
    "axes.margins = 0\n",
    "axes.set_xlabel('Date/Time')\n",
    "axes.set_ylabel('Num of Tweets')\n",
    "axes.set_title(\n",
    "    'Number of Tweets Over Time according to a Car Manufacturer- ALL TWEETS')\n",
    "axes.set_xlim(0, len(car_maker_results_list[0]))\n",
    "axes.legend(loc=\"upper right\", labels=car_makers_name_list)\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the peak of tweets for VW\n",
    "### September 15, 2015 - October 15, 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tweets_debacle = df_tweets_car_maker.filter(\n",
    "    (df_tweets_car_maker[\"VW\"] == True) & (\n",
    "        df_tweets_car_maker.POSTING_TIME > '2015-09-15 0.0.0') & (\n",
    "        df_tweets_car_maker.POSTING_TIME < '2015-10-15 0.0.0'))\n",
    "if DEBUG:\n",
    "    df_tweets_debacle.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(STOPWORDS)\n",
    "stop_words.add(\"says\")\n",
    "stop_words.add(\"over\")\n",
    "stop_words.add(\"will\")\n",
    "\n",
    "tagsRDD = df_tweets_debacle.flatMap(\n",
    "    lambda t: re.split(\"\\s\", t.MESSAGE_BODY)).filter(\n",
    "    lambda word: not word.startswith(\"http\") and \n",
    "    all(ord(c) < 128 for c in word) and \n",
    "    word not in stop_words and\n",
    "    len(word) > 3).map(\n",
    "    lambda word: (word, 1)).reduceByKey(add, 10).map(\n",
    "    lambda (a, b): (b, a)).sortByKey(False).map(\n",
    "    lambda (a, b): (b, a))\n",
    "\n",
    "# Get top words for word cloud (stopwords, etc. were removed above)\n",
    "toptags = tagsRDD.take(20)\n",
    "\n",
    "# Take 'Volkswagen' out of the pie chart keywords and then keep top ten.\n",
    "top10tags = [t for t in toptags if 'Volkswagen' not in t[0]]\n",
    "\n",
    "params = plt.gcf()\n",
    "plSize = params.get_size_inches()\n",
    "params.set_size_inches((plSize[0] * 2, plSize[1] * 2))\n",
    "\n",
    "labels = [i[0] for i in top10tags]\n",
    "sizes = [int(i[1]) for i in top10tags]\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral', \"beige\",\n",
    "          \"paleturquoise\", \"pink\", \"lightyellow\", \"coral\"]\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True,\n",
    "        startangle=90)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "word_cloud = WordCloud(background_color='white',\n",
    "                       normalize_plurals=True,\n",
    "                       collocations=True).fit_words(dict(toptags))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_maker_results_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis by car maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print('Calculating the number of positive and negative tweets over 2015 '\n",
    "          'for each car maker:')\n",
    "\n",
    "car_maker_results_list = []\n",
    "for car_maker in car_makers_name_list:\n",
    "\n",
    "    df_car_maker = df_tweets_car_maker.filter(\n",
    "        df_tweets_car_maker[car_maker] == True)\n",
    "    \n",
    "    time_sentiment_car_maker_data = df_car_maker.groupBy(\n",
    "        'POSTING_TIME', 'SENTIMENT').agg(\n",
    "        F.count('MESSAGE_ID').alias('COUNT')).orderBy(\n",
    "        'POSTING_TIME', ascending=True)\n",
    "    \n",
    "    time_sentiment_car_maker_data.cache()\n",
    "    \n",
    "    time_pos_sentiment_car_maker_data = time_sentiment_car_maker_data.filter(\n",
    "        time_sentiment_car_maker_data['SENTIMENT'] == 'POSITIVE')\n",
    "    time_neg_sentiment_car_maker_data = time_sentiment_car_maker_data.filter(\n",
    "        time_sentiment_car_maker_data['SENTIMENT'] == 'NEGATIVE')\n",
    "\n",
    "    p_time_pos_sentiment_maker = time_pos_sentiment_car_maker_data.toPandas()\n",
    "    p_time_neg_sentiment_maker = time_neg_sentiment_car_maker_data.toPandas()\n",
    "    \n",
    "    # Collect results\n",
    "    car_maker_results_list.append([p_time_pos_sentiment_maker,\n",
    "                                   p_time_neg_sentiment_maker])\n",
    "    time_sentiment_car_maker_data.unpersist()\n",
    "    \n",
    "    # Print to show progress\n",
    "    if DEBUG:\n",
    "        print('Done for ' + car_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask_1day = p_df_tweets_over_time['POSTING_TIME'].map(lambda x: x.day) == 1\n",
    "x = p_df_tweets_over_time[mask_1day]['POSTING_TIME']\n",
    "\n",
    "for i in range(0, len(car_makers_name_list)):\n",
    "    car_maker_results_list[i][0] = addMissingDates(\n",
    "        p_df_tweets_over_time, car_maker_results_list[i][0])\n",
    "    car_maker_results_list[i][1] = addMissingDates(\n",
    "        p_df_tweets_over_time, car_maker_results_list[i][1])\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(car_maker_results_list), ncols=1, figsize=(20, 15))\n",
    "colors = ['blue', 'red', 'green', 'yellow', 'black']\n",
    "\n",
    "\n",
    "for i in range(0, len(car_makers_name_list)):\n",
    "    y1 = car_maker_results_list[i][0]['COUNT']\n",
    "    y2 = car_maker_results_list[i][1]['COUNT']\n",
    "    \n",
    "    axes[i].plot(range(len(y1)), y1, linewidth=2, color='green')\n",
    "    axes[i].plot(range(len(y2)), y2, linewidth=1, color='red')\n",
    "    axes[i].set_xticks(x.index.tolist())\n",
    "    axes[i].set_xticklabels([day.strftime(\"%Y-%m-%d\") for day in x])\n",
    "    axes[i].margins = 0\n",
    "    axes[i].set_xlabel('Date/Time')\n",
    "    axes[i].set_ylabel('Num of Tweets')\n",
    "    axes[i].set_title('Number of Tweets about ' + car_makers_name_list[i] + \n",
    "                      ' Over Time - Positive and Negative')\n",
    "    axes[i].set_xlim(0, len(car_maker_results_list[0][0]))\n",
    "    axes[i].legend(loc=\"upper right\", labels=['Positive', 'Negative'])\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tweet distribution by car manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code cell calculates the number of tweets about certain car\n",
    "# manufacturers\n",
    "\n",
    "if DEBUG:\n",
    "    print('Calculating the number of tweets that mention one of the car '\n",
    "          'makers:')\n",
    "\n",
    "car_maker_tweets_count = []\n",
    "for car_maker in car_makers_name_list:\n",
    "    df_car_maker = df_tweets_car_maker.filter(\n",
    "        df_tweets_car_maker[car_maker] == True)\n",
    "    car_maker_tweets_count.append(df_car_maker.count())\n",
    "    # Print to show progress\n",
    "    if DEBUG:\n",
    "        print('Done for ' + car_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the resulting numbers in a bar chart and as percentages in a pie chart. \n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(23, 8))\n",
    "\n",
    "axes[0].bar(ind, car_maker_tweets_count, width, color='b', align='center')\n",
    "axes[0].set_ylabel('Num Tweets')\n",
    "axes[0].set_title('Number of tweets that mention a certain Car Manufacturer')\n",
    "axes[0].set_xticks(ind)\n",
    "axes[0].set_xticklabels(car_makers_name_list)\n",
    "\n",
    "# Plot\n",
    "axes[1].pie(car_maker_tweets_count,\n",
    "            autopct='%1.1f%%',\n",
    "            labels=car_makers_name_list)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].set_title('Percentage of tweets that mention a certain Car '\n",
    "                  'Manufacturer')\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distribution by sentiment by car maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the number of positive and negative tweets by car maker\n",
    "\n",
    "positive_sum_car_makers = []\n",
    "negative_sum_car_makers = []\n",
    "\n",
    "for i in range(0, len(car_makers_name_list)):\n",
    "    car_maker = car_makers_name_list[i]\n",
    "    car_maker_results_list[i][0] = car_maker_results_list[i][0].fillna(0)\n",
    "    car_maker_results_list[i][1] = car_maker_results_list[i][1].fillna(0)\n",
    "    positive_sum_car_makers.extend(\n",
    "        [car_maker_results_list[i][0]['COUNT'].sum()])\n",
    "    negative_sum_car_makers.extend(\n",
    "        [car_maker_results_list[i][1]['COUNT'].sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the results next to each other.\n",
    "\n",
    "sum_value_P_N = np.add(positive_sum_car_makers, negative_sum_car_makers)\n",
    "competitors_list_rest = np.subtract(car_maker_tweets_count, sum_value_P_N)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(23, 8))\n",
    "\n",
    "axes[0].bar(ind, positive_sum_car_makers, width, color='b', align='center')\n",
    "axes[0].bar(ind, negative_sum_car_makers, width, color='r',\n",
    "            bottom=positive_sum_car_makers, align='center')\n",
    "axes[0].bar(ind, competitors_list_rest, width, color='gray', align='center',\n",
    "            bottom=sum_value_P_N)\n",
    "axes[0].set_ylabel('Num Tweets')\n",
    "axes[0].set_title('Number of tweets that mention a certain Car Manufacturer')\n",
    "axes[0].set_xticks(ind)\n",
    "axes[0].set_xticklabels(car_makers_name_list)\n",
    "axes[0].legend(loc=\"upper left\", labels=['POSITIVE', 'NEGATIVE', 'Others'])\n",
    "\n",
    "axes[1].bar(ind, positive_sum_car_makers, width, color='b', align='center')\n",
    "axes[1].bar(ind, negative_sum_car_makers, width, color='r',\n",
    "            bottom=positive_sum_car_makers, align='center')\n",
    "axes[1].set_ylabel('Num Tweets')\n",
    "axes[1].set_title('Positive / Negative tweets - Car Manufacturer')\n",
    "axes[1].set_xticks(ind)\n",
    "axes[1].set_xticklabels(car_makers_name_list)\n",
    "axes[1].legend(loc=\"upper left\", labels=['POSITIVE', 'NEGATIVE'])\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender distribution \n",
    "\n",
    "Another interesting insight when analyzing tweets about certain\n",
    "car manufacturers and what car manufacturers might want to pay\n",
    "more attention to for marketing purposes is the distribution of\n",
    "tweets between male and female users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the gender distribution for each car maker.\n",
    "\n",
    "if DEBUG:\n",
    "    print('Calculating the distribution of male and female in tweets over'\n",
    "          ' 2015 for each competitor:')\n",
    "\n",
    "car_maker_info_list_M_F = []\n",
    "\n",
    "for car_maker in car_makers_name_list:\n",
    "    df_car_maker = df_tweets_car_maker.filter(\n",
    "        df_tweets_car_maker[car_maker] == True)\n",
    "    car_maker_gender_data = df_car_maker.groupBy('USER_GENDER').agg(\n",
    "        F.count('MESSAGE_ID').alias('COUNT'))\n",
    "    car_maker_gender_data.cache()\n",
    "\n",
    "    p_car_maker_gender_data_male = car_maker_gender_data.filter(\n",
    "        car_maker_gender_data['USER_GENDER'] == 'male').toPandas()\n",
    "    p_car_maker_gender_data_female = car_maker_gender_data.filter(\n",
    "        car_maker_gender_data['USER_GENDER'] == 'female').toPandas()\n",
    "    car_maker_info_list_M_F.append([p_car_maker_gender_data_male,\n",
    "                                    p_car_maker_gender_data_female])\n",
    "    car_maker_gender_data.unpersist()\n",
    "    # Print to show status\n",
    "    if DEBUG:\n",
    "        print('Done for ' + car_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "car_maker_list_female = []\n",
    "car_maker_list_male = []\n",
    "for i in range(0, len(car_maker_info_list_M_F)):\n",
    "    car_maker_list_female.append(0 if car_maker_info_list_M_F[i][1][\n",
    "        'COUNT'].empty else car_maker_info_list_M_F[i][1]['COUNT'][0])\n",
    "    car_maker_list_male.append(0 if car_maker_info_list_M_F[i][0][\n",
    "        'COUNT'].empty else car_maker_info_list_M_F[i][0]['COUNT'][0])\n",
    "\n",
    "sum_value_M_F = np.add(car_maker_list_male, car_maker_list_female)\n",
    "car_maker_list_M_F_rest = np.subtract(car_maker_tweets_count, sum_value_M_F)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(23, 8))\n",
    "\n",
    "axes[0].bar(ind, car_maker_list_male, width, color='g', align='center')\n",
    "axes[0].bar(ind, car_maker_list_female, width, color='b',\n",
    "            bottom=car_maker_list_male, align='center')\n",
    "axes[0].bar(ind, car_maker_list_M_F_rest, width, color='gray', align='center',\n",
    "            bottom=sum_value_M_F)\n",
    "axes[0].set_ylabel('Num Tweets')\n",
    "axes[0].set_title('Number of tweets that mention a certain Competitor')\n",
    "axes[0].set_xticks(ind)\n",
    "axes[0].set_xticklabels(car_makers_name_list)\n",
    "axes[0].legend(loc=\"upper left\", labels=['MALE', 'FEMALE', 'UNKNOWN'])\n",
    "\n",
    "axes[1].bar(ind, car_maker_list_male, width, color='g', align='center')\n",
    "axes[1].bar(ind, car_maker_list_female, width, color='b',\n",
    "            bottom=car_maker_list_male, align='center')\n",
    "axes[1].set_ylabel('Num Tweets')\n",
    "axes[1].set_title('Male / Female Distribution - Competitors')\n",
    "axes[1].set_xticks(ind)\n",
    "axes[1].set_xticklabels(car_makers_name_list)\n",
    "axes[1].legend(loc=\"upper left\", labels=['MALE', 'FEMALE'])\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring influence\n",
    "\n",
    "In this section, we will calculate the average value of the influence\n",
    "variable of the people who tweeted about a certain car maker. The\n",
    "influence varable is calculated by:\n",
    "\n",
    "$$Influence = ( \\space num \\space of \\space followers +\n",
    "\\space number \\space of \\space friends \\space ) \\div 2$$\n",
    "\n",
    "The influence score gives an indication whether someone is a famous\n",
    "person or a public figure in society or whether the twitter account\n",
    "is owned by the media or a company. This average value gives an\n",
    "indication about the people who are interested in a certain car maker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns a list of pandas DFs.\n",
    "def getInsights_Influence(sparkDF, car_maker_list):\n",
    "    if DEBUG:\n",
    "        print('Calculating the influence insight of the users in tweets over '\n",
    "              '2015 for each competitors:')\n",
    "\n",
    "    car_maker_result_list = []\n",
    "    for car_maker in car_maker_list:\n",
    "        df_car_maker = sparkDF.filter(sparkDF[car_maker] == True)\n",
    "        car_maker_insight_data = df_car_maker.select(F.avg('INFLUENCE').alias(\n",
    "            'AVE_INFLUENCE'))\n",
    "        car_maker_result_list.append(car_maker_insight_data.toPandas())\n",
    "        df_car_maker.unpersist()\n",
    "        car_maker_insight_data.unpersist()\n",
    "        # Print to show status\n",
    "        if DEBUG:\n",
    "            print('Done for ' + car_maker)\n",
    "    return car_maker_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "influenceInsights = getInsights_Influence(df_tweets_car_maker,\n",
    "                                          car_makers_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "influence_list = []\n",
    "for i in range(0, len(car_makers_name_list)):\n",
    "    influence_list.append(influenceInsights[i]['AVE_INFLUENCE'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 8))\n",
    "\n",
    "rects = axes.bar(ind, influence_list, width, color='b', align='center')\n",
    "axes.set_ylabel('Average Value of the Influence Score')\n",
    "\n",
    "# influence=(number of friends + number of followers)/2\n",
    "axes.set_title('The Average of the User Influence for Each Car Maker')\n",
    "axes.set_xticks(ind)\n",
    "axes.set_xticklabels(car_makers_name_list)\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "influenceInsights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of tweets by country across car manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell calculates in which countries the most tweets were posted about\n",
    "# car manufacturers. This information can support marketing and sales when\n",
    "# evaluating potential customers.\n",
    "\n",
    "if DEBUG:\n",
    "    print('Calculating the country distribution of tweets over 2015 for each '\n",
    "          'car maker:')\n",
    "\n",
    "car_maker_info_list_countries = []\n",
    "i = 0\n",
    "\n",
    "for car_maker in car_makers_name_list:\n",
    "    df_car_maker = df_tweets_car_maker.filter(\n",
    "        df_tweets_car_maker[car_maker] == True)\n",
    "    car_maker_country_data = df_car_maker.groupBy(\n",
    "        'USER_COUNTRY').agg(\n",
    "        F.count('MESSAGE_ID').alias('COUNT')).orderBy(\n",
    "        'COUNT', ascending=False)\n",
    "    car_maker_country_data.cache()\n",
    "\n",
    "    p_car_maker_country_data = car_maker_country_data.toPandas()\n",
    "    car_maker_info_list_countries.append(p_car_maker_country_data)\n",
    "    car_maker_country_data.unpersist()\n",
    "\n",
    "    # Print to show status\n",
    "    if DEBUG:\n",
    "        print('Done for ' + car_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_taken_countries = 5\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(car_makers_name_list), ncols=1, figsize=(15, 20))\n",
    "\n",
    "\n",
    "for j in range(0, len(car_makers_name_list)):\n",
    "    color = 'b'  # np.random.rand(3,1)\n",
    "    colors = np.repeat(color, num_taken_countries).tolist()\n",
    "\n",
    "    country_list_num = car_maker_info_list_countries[j]['COUNT'][\n",
    "        :num_taken_countries]\n",
    "    country_list_labels = car_maker_info_list_countries[j]['USER_COUNTRY'][\n",
    "        :num_taken_countries]\n",
    "\n",
    "    for counter in range(0, len(country_list_labels)):\n",
    "        if country_list_labels[counter] == 'NONE':\n",
    "            colors[counter] = 'gray'\n",
    "\n",
    "    axes[j].barh(np.arange(num_taken_countries),\n",
    "                 country_list_num,\n",
    "                 width,\n",
    "                 color=colors,\n",
    "                 align='center')\n",
    "    axes[j].set_xlabel('Num Tweets')\n",
    "    axes[j].set_title('Country Distribution for ' + car_makers_name_list[j])\n",
    "    axes[j].set_yticks(ind)\n",
    "    axes[j].set_yticklabels(country_list_labels.tolist())\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for each selected country.\n",
    "us_list = []\n",
    "uk_list = []\n",
    "de_list = []\n",
    "\n",
    "\n",
    "def getListForCountry(x, country):\n",
    "    df = x[x['USER_COUNTRY'] == country]['COUNT']\n",
    "    return [0] if df.empty else [df.tolist()[0]]\n",
    "\n",
    "\n",
    "for i in range(0, len(car_makers_name_list)):\n",
    "    x = car_maker_info_list_countries[i]\n",
    "    us_list.extend(getListForCountry(x, 'UNITED STATES'))\n",
    "    uk_list.extend(getListForCountry(x, 'UNITED KINGDOM'))\n",
    "    de_list.extend(getListForCountry(x, 'GERMANY'))\n",
    "\n",
    "# Plot\n",
    "colors = ['r', 'g', 'b', 'w', 'pink', 'y']\n",
    "us_values = us_list\n",
    "uk_values = uk_list\n",
    "de_values = de_list\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(23, 10))\n",
    "\n",
    "axes[0].pie(us_values,\n",
    "            autopct='%1.1f%%', colors=colors,\n",
    "            labels=car_makers_name_list)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].set_title('Percentage of Tweets from Users in US among Car Makers')\n",
    "\n",
    "# Plot\n",
    "axes[1].pie(uk_values,\n",
    "            autopct='%1.1f%%',\n",
    "            colors=colors,\n",
    "            labels=car_makers_name_list)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].set_title('Percentage of Tweets from Users in UK among Car Makers')\n",
    "\n",
    "# Plot\n",
    "axes[2].pie(de_values,\n",
    "            autopct='%1.1f%%', colors=colors, labels=car_makers_name_list)\n",
    "axes[2].set_aspect('equal')\n",
    "axes[2].set_title(\n",
    "    'Percentage of Tweets from Users in GERMANY among Car Makers')\n",
    "\n",
    "fig.subplots_adjust(hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary\n",
    "In this notebook you learned how to use notebooks to analyze Twitter\n",
    "data and extract interesting insights from tweets. You learned how to\n",
    "easily perform complex computations on a large amount of data in a\n",
    "notebook by using SparkContext, which enables you to start tasks on the\n",
    "Spark cluster. In addition, you learned how to integrate data from dashDB\n",
    "using the Spark connector and how to use Spark and pandas DataFrames."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}